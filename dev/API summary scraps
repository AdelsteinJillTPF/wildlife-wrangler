
        """
        # Rename some columns so they will match up with the output schema
        insertDF.rename({"issue": "issues", 'id': 'record_id'}, axis=1, inplace=True)

        # Fields that will be int or float need to be converted from "UNKNOWN"
        dfRaw = insertDF.replace({"coordinateUncertaintyInMeters": {"UNKNOWN": np.nan},
                       "radius_meters": {"UNKNOWN": np.nan},
                       "individualCount": {"UNKNOWN": 1},
                       "weight": {"UNKOWN": 10},
                       "detection_distance_m": {"UNKNOWN": 0}})
        print(dfRaw.dtypes)
        """
        """
        # Append to the output template
        dfRaw = (pd.DataFrame(columns=output_schema.keys())
                 .append(insertDF, ignore_index=True, sort=False))
        print(dfRaw.head(1).T)"""
                                                                                 #df0.drop(["issue", "id"], inplace=True, axis=1)


        """        dfRaw['coordinateUncertaintyInMeters'].replace(to_replace="UNKNOWN",
                                                     value=np.NaN, inplace=True)              # DELETE THIS???????
        #df0 = df0.astype({'coordinateUncertaintyInMeters': 'float',
        #                  'decimalLatitude': 'string', 'decimalLongitude': 'string'})
        dfRaw['individualCount'].replace(to_replace="UNKNOWN", value=1,
                                       inplace=True)
        dfRaw["weight"].repalce(to_replace="UNKNOWN", value=np.nan,
                                              inplace=True)
        dfRaw["detection_distance_m"].repalce(to_replace="UNKNOWN",
                                              value=np.nan, inplace=True)"""

# Summarize the attributes that were returned
#    Count entries per attribute(column), reformat as new df with appropriate
#   columns.  Finally, insert into database.
#    NOTE: When pulling from df0copy, only a specified subset of keys are
#    assessed (output_schema.keys()).  For a more complete picture, all_jsons must be
#   assessed.  That has historically been very slow.

""" # Fastest, but least informative method for gbif_fields_returned
  newt = datetime.now()
  df0copy = df0.copy()
  df0copy.where(df0copy != 'UNKNOWN', inplace=True)
  df_populated1 = pd.DataFrame(df0copy.count(axis=0).T.iloc[1:])
  #df_populated1['included(n)'] = df_populated1[0] # Can this be determined from all_jsons?  Quickly?
  df_populated1['populated(n)'] = df_populated1[0]
  df_populated2 = df_populated1.filter(items=['included(n)', 'populated(n)'], axis='columns')
  df_populated2.index.name = 'attribute'
  df_populated2.to_sql(name='gbif_fields_returned', con=conn, if_exists='replace')
  print("Summarized fields returned: " + str(datetime.now() - newt))
  """
  # Slower, but more informative method for gbif_fields_returned
  '''
  The method below provides more information on values returned than the
  one above, but is slow.  Can it be improved to be faster?
  '''
  """
  keys = [list(x.keys()) for x in all_jsons]
  keys2 = set([])
  for x in keys:
      keys2 = keys2 | set(x)
  dfK = pd.DataFrame(index=keys2, columns=['included(n)', 'populated(n)'])

  #dfK = pd.DataFrame(index=output_schema.keys(), columns=['included(n)', 'populated(n)'])
  dfK['included(n)'] = 0
  dfK['populated(n)'] = 0
  timestamp = datetime.now()
  for t in all_jsons: # Start of slow
      for y in t.keys():
          dfK.loc[y, 'included(n)'] += 1
          try:
              int(t[y])
              dfK.loc[y, 'populated(n)'] += 1
          except:
              if t[y] == None:
                  pass
              elif len(t[y]) > 0:
                  dfK.loc[y, 'populated(n)'] += 1 # End of slow
  print("Summarized fields returned: " + str(datetime.now() - timestamp))
  dfK.sort_index(inplace=True)
  dfK.index.name = 'attribute'

  # Save attribute summary into the output database
  dfK.to_sql(name='gbif_fields_returned', con=conn, if_exists='replace')"""
