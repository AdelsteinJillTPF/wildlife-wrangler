{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of using all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some paths and names in the cell below.\n",
    "config_path = \"T:/Data/\"  # Path to folder where you saved your wildlifeconfig file.\n",
    "filter_set_json = None\n",
    "taxon_json = None\n",
    "query_name = 'All_params_1'\n",
    "ask_eBird = True\n",
    "ask_GBIF = False\n",
    "get_dwca = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run 2021-05-28 14:31:12.428443\n",
      "Results were saved in T:/Occurrence_Records/All_params_1.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Nothing to fill out in this cell\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sqlite3\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(config_path)\n",
    "import wranglerconfig as config\n",
    "sys.path.append(config.codeDir)\n",
    "import wrangler_functions as functions\n",
    "\n",
    "# Define some variables\n",
    "t1 = datetime.now()\n",
    "working_directory = config.workDir\n",
    "username = config.gbif_username\n",
    "password = config.gbif_password\n",
    "email = config.gbif_email\n",
    "EBD_file = config.EBD_file\n",
    "output_database = working_directory + query_name + '.sqlite'\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.width', 600)\n",
    "pd.set_option('display.max_colwidth', 75)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "print(\"Notebook run \" + str(t1))\n",
    "print(\"Results were saved in \" + output_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxon Concept\n",
    "Caution! -- Taxon concept mismatches can cause inclusion of innapropriate records or ommission of useful records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_taxon_id = \"bwitux0\"\n",
    "gbif_id = 9606290 \n",
    "ebird_id = \"Wild Turkey\"\n",
    "detection_distance_m = 300\n",
    "taxon_polygon = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EBIRD_ID': 'Wild Turkey',\n",
      " 'GBIF_ID': 9606290,\n",
      " 'ID': 'bwitux0',\n",
      " 'TAXON_EOO': None,\n",
      " 'detection_distance_m': 300}\n"
     ]
    }
   ],
   "source": [
    "# If a json was provided, use it, otherwise create a new one with info that was provided.\n",
    "if taxon_json is None:\n",
    "    # Build a species dictionary\n",
    "    taxon_info = {\"ID\": your_taxon_id, \"GBIF_ID\": gbif_id, \"EBIRD_ID\": ebird_id, \"detection_distance_m\": detection_distance_m,\n",
    "                  \"TAXON_EOO\": taxon_polygon}\n",
    "\n",
    "    # Save as json object\n",
    "    out_file = open(working_directory + your_taxon_id + \".json\", \"w\")  \n",
    "    json.dump(taxon_info, out_file) \n",
    "    out_file.close() \n",
    "\n",
    "if taxon_json is not None:\n",
    "    with open(taxon_json, \"r\") as f:\n",
    "        taxon_info = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "pprint.pprint(taxon_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Set\n",
    "Specify how you want records filtered and cleaned.  Alternatively, you can load a filter set here by specifying a path in the first cell of this notebook.  \n",
    "\n",
    "To skip a filter, enter \"None\" without the quotation marks or \"\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter set name: Test_EBD_1\n"
     ]
    }
   ],
   "source": [
    "filter_set_name = \"Test_EBD_1\"\n",
    "print(\"Filter set name: \" + str(filter_set_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBIF Request Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request a Darwin Core Archive? False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Whether to get records from GBIF in a darwin core archive.  \n",
    "\"False\" uses the GBIF API, which has limitations that may be important.  \n",
    "\"True\" requests results be emailed in a darwin core archive.\n",
    "\"\"\"\n",
    "print(\"Request a Darwin Core Archive? \" + str(get_dwca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Limits\n",
    "Notes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years: 2018,2021\n",
      "Months: 5,12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Enter year and month ranges.  For example, years_range = 2015,2017 and months_range = 3,6\n",
    "'''\n",
    "years_range = \"2018,2021\"\n",
    "months_range = \"5,12\"\n",
    "print(\"Years: \" + str(years_range))\n",
    "print(\"Months: \" + str(months_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n"
     ]
    }
   ],
   "source": [
    "country = \"US\"\n",
    "print(\"Country: \" + country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounding Box\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude range: 20,50\n",
      "Longitude range: -130,-67\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Coordinates should correspond to WGS84 (EPSG:4326).  Don't use this option if you specify a query polygon below.\n",
    "'''\n",
    "lat_range = \"20,50\"\n",
    "lon_range = \"-130,-67\"\n",
    "print(\"Latitude range: \" + lat_range)\n",
    "print(\"Longitude range: \" + lon_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area of Interest\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Specify polygons to use for spatial filtering.  \n",
    "Records with coordinates outside of the polygons will be removed.  \n",
    "You can specify a geometry for the query and one for the species.  \n",
    "The species geometry is included to facilitate better handling of taxonomic issues.  \n",
    "If both are provided, the intersection is calculated and used as the filter.  \n",
    "The format should be well known text in WGS84 (EPSG 4326), and very importantly, vertices need to be listed counter-clockwise.  \n",
    "See the ccw_wkt_from_shp() function in wrangler functions for help.\n",
    "'''\n",
    "query_polygon = None\n",
    "print(query_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxon EOO\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use taxon extent of occurrence? False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "True or False whether you want to apply the taxon EOO to the filtering.  \n",
    "If True, removes records with centroids outside of the extent of occurrence geometry you provided in taxon_info.\n",
    "'''\n",
    "use_taxon_geometry = False\n",
    "print(\"Use taxon extent of occurrence? \" + str(use_taxon_geometry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geoissue\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with geoissues OK? None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Are GBIF records with noted geoissues OK to include? GBIF only.\n",
    "'''\n",
    "geoissue = None\n",
    "print(\"Records with geoissues OK? \" + str(geoissue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collections\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['naturgucker', 'VBBA2']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List collection codes that you'd like to omit. GBIF only.\n",
    "'''\n",
    "collection_codes_omit = [\"naturgucker\", \"VBBA2\"]\n",
    "print(\"Omit: \" + str(collection_codes_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Institutions\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['e3aa237e-53c8-4c46-93ef-77a18bb9c80e', 'urn:lsid:biocol.org:col:34939']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List institution codes that you'd like to omit. GBIF only\n",
    "'''\n",
    "institutions_omit = [\"e3aa237e-53c8-4c46-93ef-77a18bb9c80e\", \"urn:lsid:biocol.org:col:34939\"]\n",
    "print(\"Omit: \" + str(institutions_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['Gaia Guide', 'Earth Guardians Weekly Feed']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List datasets that you'd like to omit.\n",
    "'''\n",
    "datasets_omit = [\"Gaia Guide\", \"Earth Guardians Weekly Feed\"]\n",
    "print(\"Omit: \" + str(datasets_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinate Uncertainty\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate uncertainty required? True\n",
      "Default coordinate uncertainty to use: 2000\n",
      "Maximum allowable coordinate uncertainty: 10000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Do you want to remove records without coordinate uncertainty (True) or leave them in the data set (False)?  \n",
    "Note that eBird records in GBIF (EOD) do not have this and neither do data in the EBD dataset.  \n",
    "With the EBD, the length of traveling counts is used as a surrogate value.  \n",
    "\n",
    "max_coordinate_uncertainty must be an integer greater than 0.\n",
    "\n",
    "default_coordUncertainty -- coordinateUncertaintyInMeters is often not provided.  \n",
    "Here is an option to use a default.  If you don't want anything entered, set this equal to False (boolean, not string).\n",
    "\n",
    "A maximum for coordinate uncertainty can also be set in meters.\n",
    "'''\n",
    "has_coordinate_uncertainty = True\n",
    "default_coordUncertainty = 2000\n",
    "max_coordinate_uncertainty = 10000\n",
    "print(\"Coordinate uncertainty required? \" + str(has_coordinate_uncertainty))\n",
    "print(\"Default coordinate uncertainty to use: \" + str(default_coordUncertainty))\n",
    "print(\"Maximum allowable coordinate uncertainty: \" + str(max_coordinate_uncertainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bases\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['PRESERVED_SPECIMEN', 'MACHINE_OBSERVATION']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List bases of records that you want to omit.  GBIF only.\n",
    "'''\n",
    "bases_omit = [\"PRESERVED_SPECIMEN\", \"MACHINE_OBSERVATION\"]\n",
    "print(\"Omit: \" + str(bases_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Protocols\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['eBird Pelagic Protocol', 'Random']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List sampling protocols that you would like to omit.\n",
    "'''\n",
    "sampling_protocols_omit = [\"eBird Pelagic Protocol\", \"Random\"]\n",
    "print(\"Omit: \" + str(sampling_protocols_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omit: ['TAXON_MATCH_HIGHERRANK', 'COORDINATE_UNCERTAINTY_METERS_INVALID']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List issues that you want to omit.  GBIF only.\n",
    "'''\n",
    "issues_omit = [\"TAXON_MATCH_HIGHERRANK\", \"COORDINATE_UNCERTAINTY_METERS_INVALID\"]\n",
    "print(\"Omit: \" + str(issues_omit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allow duplicates? False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Specify whether duplicates on latitude, longitude, and date should be included.\n",
    "'''\n",
    "duplicates_OK = False\n",
    "print(\"Allow duplicates? \" + str(duplicates_OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Set Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bases_omit': ['PRESERVED_SPECIMEN', 'MACHINE_OBSERVATION'],\n",
      " 'collection_codes_omit': ['naturgucker', 'VBBA2'],\n",
      " 'country': 'US',\n",
      " 'datasets_omit': ['Gaia Guide', 'Earth Guardians Weekly Feed'],\n",
      " 'default_coordUncertainty': 2000,\n",
      " 'duplicates_OK': False,\n",
      " 'geoissue': None,\n",
      " 'get_dwca': False,\n",
      " 'has_coordinate_uncertainty': True,\n",
      " 'institutions_omit': ['e3aa237e-53c8-4c46-93ef-77a18bb9c80e',\n",
      "                       'urn:lsid:biocol.org:col:34939'],\n",
      " 'issues_omit': ['TAXON_MATCH_HIGHERRANK',\n",
      "                 'COORDINATE_UNCERTAINTY_METERS_INVALID'],\n",
      " 'lat_range': '20,50',\n",
      " 'lon_range': '-130,-67',\n",
      " 'max_coordinate_uncertainty': 10000,\n",
      " 'months_range': '5,12',\n",
      " 'name': 'Test_EBD_1',\n",
      " 'query_polygon': None,\n",
      " 'sampling_protocols_omit': ['eBird Pelagic Protocol', 'Random'],\n",
      " 'use_taxon_geometry': False,\n",
      " 'years_range': '2018,2021'}\n"
     ]
    }
   ],
   "source": [
    "if filter_set_json is None:\n",
    "    # Build a filter set dictionary\n",
    "    filter_set = {\"name\": filter_set_name, \"query_polygon\": query_polygon, \"issues_omit\": issues_omit,\n",
    "                  \"sampling_protocols_omit\": sampling_protocols_omit, \"bases_omit\": bases_omit,\n",
    "                  \"has_coordinate_uncertainty\": has_coordinate_uncertainty, \"geoissue\": geoissue,\n",
    "                  \"default_coordUncertainty\": default_coordUncertainty,\n",
    "                  \"max_coordinate_uncertainty\": max_coordinate_uncertainty,\n",
    "                  \"datasets_omit\": datasets_omit, \"collection_codes_omit\": collection_codes_omit,\n",
    "                  \"institutions_omit\": institutions_omit, \"geoissue\": geoissue, \"use_taxon_geometry\": use_taxon_geometry,\n",
    "                  \"lat_range\": lat_range, \"lon_range\": lon_range, \"country\": country, \n",
    "                  \"years_range\": years_range, \"months_range\": months_range, \"duplicates_OK\": duplicates_OK, \"get_dwca\": get_dwca}\n",
    "    \n",
    "    # Replace empty strings with None\n",
    "    for x in filter_set.keys():\n",
    "        if filter_set[x] == \"\":\n",
    "            filter_set[x] = None\n",
    "    \n",
    "    # Save as json object\n",
    "    with open(working_directory + filter_set_name + \".json\", \"w\") as f:\n",
    "        json.dump(filter_set, f) \n",
    "        f.close()\n",
    "        \n",
    "if filter_set_json is not None:\n",
    "    with open(filter_set_json, \"r\") as f:\n",
    "        filter_set = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "    # Replace empty strings with None\n",
    "    for x in filter_set.keys():\n",
    "        if filter_set[x] == \"\":\n",
    "            filter_set[x] = None\n",
    "        \n",
    "pprint.pprint(filter_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output database\n",
    "functions.build_output_database(output_database)\n",
    "\n",
    "# Save taxon and filter set info into database\n",
    "output_db_conn= sqlite3.connect(output_database)\n",
    "cursor = output_db_conn.cursor()\n",
    "pd.DataFrame(taxon_info.values(), taxon_info.keys()).applymap(str).to_sql(name='taxon_concept', con=output_db_conn, if_exists='replace')\n",
    "pd.DataFrame(filter_set.values(), filter_set.keys()).applymap(str).to_sql(name='filter_set', con=output_db_conn, if_exists='replace')\n",
    "output_db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran EBD query with Auk: 0:27:23.619058\n",
      "Calculated the spatial filter polygon: 0:00:02.007671\n",
      "Applied spatial filter: 0:00:00\n",
      "Summarized fields returned: 0:00:00.186977\n",
      "Prepared the eBird records for processing: 0:00:06.251169\n"
     ]
    }
   ],
   "source": [
    "# Run the appropriate queries\n",
    "if ask_eBird == True and ask_GBIF == True:\n",
    "    # Run eBird query\n",
    "    ebird_data = functions.get_EBD_records(taxon_info, filter_set, working_directory, EBD_file, query_name)\n",
    "    # Run GBIF query\n",
    "    gbif_data = functions.get_GBIF_records(taxon_info, filter_set, query_name, working_directory, username, password, email) \n",
    "\n",
    "elif ask_eBird == True and ask_GBIF == False:\n",
    "    # Run eBird query\n",
    "    ebird_data = functions.get_EBD_records(taxon_info, filter_set, working_directory, EBD_file, query_name)\n",
    "    gbif_data = None\n",
    "\n",
    "elif ask_eBird == False and ask_GBIF == True:\n",
    "    # Run GBIF query\n",
    "    gbif_data = functions.get_GBIF_records(taxon_info, filter_set, query_name, working_directory, username, password, email)\n",
    "    ebird_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out records with undesirable values, locations, and/or duplication.\n",
    "#import importlib\n",
    "#importlib.reload(functions)\n",
    "for x in filter_set.keys():\n",
    "    if filter_set[x] == \"\":\n",
    "        filter_set[x] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data frames for processing: 0:00:03.147511\n",
      "Summarized values acquired: 0:00:00.623346\n",
      "Number of georeferenced records: 163610\n",
      "Number of record without georeference: 62068\n",
      "Applying default coordinate uncertainties\n",
      "AN ERROR OCCURRED !!!!!!!!!!!!!\n",
      "Performed filtering: 0:00:01.503516\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-608877e20aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m functions.process_records(ebird_data=ebird_data, gbif_data=gbif_data, filter_set=filter_set, \n\u001b[0;32m      2\u001b[0m                         \u001b[0mtaxon_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtaxon_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworking_directory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                         query_name=query_name)\n\u001b[0m",
      "\u001b[1;32mT:/Code/wildlife-wrangler\\wrangler_functions.py\u001b[0m in \u001b[0;36mprocess_records\u001b[1;34m(ebird_data, gbif_data, filter_set, taxon_info, working_directory, query_name)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilter_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"duplicates_OK\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m         \u001b[0mdf_filterZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_duplicates_latlongdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filter3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilter_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"duplicates_OK\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:/Code/wildlife-wrangler\\wrangler_functions.py\u001b[0m in \u001b[0;36mdrop_duplicates_latlongdate\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[1;31m# Record df length before removing duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     \u001b[0minitial_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m     \"\"\"\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5860\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5861\u001b[0m                     results.append(\n\u001b[1;32m-> 5862\u001b[1;33m                         \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5863\u001b[0m                     )\n\u001b[0;32m   5864\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5877\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5878\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     def convert(\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\Miniconda3\\envs\\wrangler1\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[1;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'X'"
     ]
    }
   ],
   "source": [
    "functions.process_records(ebird_data=ebird_data, gbif_data=gbif_data, filter_set=filter_set, \n",
    "                        taxon_info=taxon_info, working_directory=working_directory, \n",
    "                        query_name=query_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_db_conn= sqlite3.connect(output_database)\n",
    "cursor = output_db_conn.cursor()\n",
    "record_count = cursor.execute(\"SELECT COUNT(record_id) FROM occurrence_records;\").fetchone()\n",
    "print(str(record_count[0]) + \" records were saved in the output database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes Returned for GBIF Records\n",
    "This count was made before filters were applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ask_GBIF == True:\n",
    "    fields_summary = pd.read_sql(\"SELECT * FROM gbif_fields_returned\", output_db_conn)\n",
    "    fields_summary.index.name = 'Field'\n",
    "    pd.set_option('display.max_rows', 250)\n",
    "    print(fields_summary.sort_values(by=\"attribute\"))\n",
    "if ask_GBIF == False:\n",
    "    print(\"GBIF was not queried.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes Returned for eBird Records\n",
    "This count was made before filters were applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_eBird == True:\n",
    "    fields_summary = pd.read_sql(\"SELECT * FROM ebird_fields_returned\", output_db_conn)\n",
    "    fields_summary.index.name = 'Field'\n",
    "    pd.set_option('display.max_rows', 250)\n",
    "    print(fields_summary)\n",
    "if ask_eBird == False:\n",
    "    print(\"No eBird Basic Dataset was queried.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = pd.read_sql(sql=\"SELECT * FROM sources;\", con=output_db_conn)\n",
    "print(sources[['institutionID', 'collectionCode', 'datasetName', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bases = pd.read_sql(sql=\"SELECT * FROM attribute_value_counts WHERE attribute = 'basisOfRecord';\", con=output_db_conn)\n",
    "print(bases[['value', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protocols = pd.read_sql(sql=\"SELECT * FROM attribute_value_counts WHERE attribute = 'samplingProtocol';\", con=output_db_conn)\n",
    "if protocols.empty == True:\n",
    "    print(\"No protocols were documented.\")\n",
    "if protocols.empty == False:\n",
    "    print(protocols[['value', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "issues = pd.read_sql(sql=\"SELECT * FROM attribute_value_counts WHERE attribute = 'issues';\", con=output_db_conn)\n",
    "if issues.empty == True:\n",
    "    print(\"No issues were documented.\")\n",
    "if issues.empty == False:\n",
    "    print(issues[['value', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishment Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment = pd.read_sql(sql=\"SELECT * FROM attribute_value_counts WHERE attribute = 'establishmentMeans';\", con=output_db_conn)\n",
    "if establishment.empty == True:\n",
    "    print(\"No establishment means were reported.\")\n",
    "if establishment.empty == False:\n",
    "    print(establishment[['value', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification Qualifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications = pd.read_sql(sql=\"SELECT * FROM attribute_value_counts WHERE attribute = 'identificationQualifers';\", con=output_db_conn)\n",
    "if qualifications.empty == True:\n",
    "    print(\"No identification qualifiers were reported.\")\n",
    "if qualifications.empty == False:\n",
    "    print(establishment[['value', 'acquired', 'removed', 'retained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptions of Retained Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the record coordinates as a data frame\n",
    "record_coordinates = (pd.read_sql(\"\"\"SELECT decimalLatitude, decimalLongitude, radius_meters\n",
    "                                     FROM occurrence_records\"\"\",\n",
    "                                  con=output_db_conn)\n",
    "                      .astype({'decimalLongitude': 'float', 'decimalLatitude': 'float',\n",
    "                               'radius_meters': 'float'}))\n",
    "\n",
    "# Make the data frame spatial\n",
    "gdf = gpd.GeoDataFrame(record_coordinates, geometry=gpd.points_from_xy(record_coordinates['decimalLongitude'],\n",
    "                                                   record_coordinates['decimalLatitude']))\n",
    "\n",
    "# Set the coordinate reference system\n",
    "gdf.crs={'init' :'epsg:4326'}\n",
    "\n",
    "# Create world map <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "base = world.plot(figsize=(12,12), color = 'darkkhaki')\n",
    "gdf.plot(ax=base, marker='o', color='k', markersize=5)\n",
    "plt.show()\n",
    "\n",
    "# Create USA map <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "if filter_set[\"country\"] == \"US\":\n",
    "    usa_bbox = np.array([-124.725839,   24.498131,  -66.949895,   49.384358])\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.set_xlim(([usa_bbox[0],  usa_bbox[2]]))\n",
    "    ax.set_ylim(([usa_bbox[1],  usa_bbox[3]]))\n",
    "    world.plot(ax=ax, color='darkkhaki')\n",
    "    gdf.plot(ax=ax, marker='o', color='k', markersize=5)\n",
    "    plt.show()\n",
    "\n",
    "# Create coordinate extent map <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "os.chdir(config.codeDir)\n",
    "states = gpd.read_file(os.getcwd() + '/data/us_states.shp')\n",
    "\n",
    "# Reproject states and record coordinates to facilitate buffering\n",
    "states = states.to_crs(epsg=5070)\n",
    "footprints = gdf.to_crs(epsg=5070)\n",
    "\n",
    "# Buffer points for record footprints\n",
    "footprints['footprint']=footprints.apply(lambda x: x.geometry.buffer(x.radius_meters), axis=1)\n",
    "footprints.set_geometry(col='footprint', inplace=True, drop=True)\n",
    "\n",
    "# Map the buffered points/footprints\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "coordinate_bbox = footprints.geometry.total_bounds\n",
    "ax.set_xlim(([coordinate_bbox[0],  coordinate_bbox[2]]))\n",
    "ax.set_ylim(([coordinate_bbox[1],  coordinate_bbox[3]]))\n",
    "states.plot(ax=ax, color = 'darkkhaki')\n",
    "footprints.boundary.plot(ax=ax, color='k')\n",
    "plt.show()\n",
    "\n",
    "# Cleanup\n",
    "del footprints, gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Years Represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_years = [int(x[0]) for x in cursor.execute(\"SELECT strftime('%Y', eventDate) FROM occurrence_records\").fetchall()]\n",
    "years = filter_set['years_range']\n",
    "years = years.split(',')\n",
    "yearsrng = list(range(int(years[0]), int(years[1]), 1))\n",
    "binsnum = int(years[1]) - int(years[0])\n",
    "plt.hist(occ_years, bins=binsnum)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.xticks(yearsrng, rotation=90)\n",
    "plt.title(\"Occurrences per Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Months Represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_months = [int(x[0]) for x in cursor.execute(\"SELECT strftime('%m', eventDate) FROM occurrence_records\").fetchall()]\n",
    "plt.hist(occ_months, bins=range(1, 14), color=\"g\")\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"month\")\n",
    "plt.xticks(range(1, 13))\n",
    "plt.title(\"Occurrences per Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Coordinate Uncertainty Values for Retained Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occ_cert = [float(x[0]) for x in cursor.execute(\"SELECT coordinateUncertaintyInMeters FROM occurrence_records\").fetchall()]\n",
    "maxi = max(occ_cert)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.hist(occ_cert, bins=50, color=\"r\")\n",
    "plt.xticks(range(0, int(maxi), int(maxi/50)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_cert = [float(x[0]) for x in cursor.execute(\"SELECT coordinateUncertaintyInMeters FROM occurrence_records\").fetchall()]\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.boxplot(occ_cert, vert=False)\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_max = 2000\n",
    "occ_cert2 = [x for x in occ_cert if x <= rng_max]\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(occ_cert2, bins=30, color=\"m\", align='mid')\n",
    "plt.xticks(range(0, rng_max + 100, int(rng_max/30.)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainties Below 2km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"General remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT general_remarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Event remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT eventRemarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Occurrence remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT occurrenceRemarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Location remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT locationRemarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identified remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT general_remarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Georeference remarks:\")\n",
    "remarks = output_db_conn.execute(\"SELECT DISTINCT georeferenceRemarks FROM occurrence_records;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "#### eBird "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_eBird == True:\n",
    "    with open(config.EBD_file[:-22] + \"recommended_citation.txt\", \"r\") as reference:\n",
    "        print(reference.readlines()[0])\n",
    "else:\n",
    "    print(\"No eBird Basic Dataset was queried\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_GBIF == True:\n",
    "    if get_dwca == True:\n",
    "        print(\"Citations-- \")\n",
    "        print(cursor.execute(\"SELECT citations FROM GBIF_download_info\").fetchall()[0][0])\n",
    "    else:\n",
    "        print(\"Set 'get_dwca' to True to acquire a list of citations\")\n",
    "if ask_GBIF == False:\n",
    "    print(\"GBIF was not queried.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_GBIF == True:\n",
    "    if get_dwca == True:\n",
    "        print(\"Rights-- \")\n",
    "        print(output_db_conn.execute(\"SELECT rights FROM GBIF_download_info\").fetchall()[0][0])\n",
    "    else:\n",
    "        print(\"Set 'get_dwca' to True to see the rights\")\n",
    "if ask_GBIF == False:\n",
    "    print(\"GBIF was not queried.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_GBIF == True:\n",
    "    if get_dwca == True:\n",
    "        print(\"DOI-- \")\n",
    "        doi = output_db_conn.execute(\"SELECT doi FROM GBIF_download_info\").fetchall()[0][0]\n",
    "        print(\"https://doi.org/\" + doi)\n",
    "    else:\n",
    "        print(\"Set 'get_dwca' to True to perform a search with a doi assigned\")\n",
    "if ask_GBIF == False:\n",
    "    print(\"GBIF was not queried.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_GBIF == True:\n",
    "    if get_dwca == True:\n",
    "        print(\"GBIF download key-- \")\n",
    "        print(output_db_conn.execute(\"SELECT download_key FROM GBIF_download_info\").fetchall()[0][0])\n",
    "    else:\n",
    "        print(\"Set 'get_dwca' to True to perform a search with a download key assigned\")\n",
    "if ask_GBIF == False:\n",
    "    print(\"GBIF was not queried\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_db_conn.close()\n",
    "del cursor\n",
    "t2 = datetime.now()\n",
    "print(t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
