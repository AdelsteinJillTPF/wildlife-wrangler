{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Species] Occurrence Records Summary\n",
    "This notebook is a tool for exploring data sets requested from GBIF (and eventually other sources), and mostly for developing criteria for filtering records (filter sets).  When the entire notebook is run, it retrieves records according to the filter sets specified and saves the results (records and some summary tables) in an sqlite database.  Some information is pulled from the parameters.sqlite database that is saved in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import geopandas as gpd\n",
    "pd.set_option('display.width', 600)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "from IPython.display import Image\n",
    "from pygbif import occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "t1 = datetime.now()\n",
    "import repo_functions as functions\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"T:/GAP/Data\")\n",
    "import gapconfig as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug with mpl_toolkits, the following code is a temp fix, hopefully.\n",
    "https://stackoverflow.com/questions/52911232/basemap-library-using-anaconda-jupyter-notebooks-keyerror-proj-lib/54087410#54087410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJ_LIB'] = r'c:\\Users\\nmtarr\\AppData\\Local\\Continuum\\miniconda3\\envs\\occurrences\\Library\\share'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variables\n",
    "Fill out stuff in this cell.  \n",
    "\n",
    "Notes:\n",
    "default_coordUncertainty -- coordinateUncertaintyInMeters is often not provided.  Here is an option to use a default.  If you don't want anything entered, set this equal to False (boolean, not string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run 2020-03-11 17:30:06.262002\n",
      "T:/temp/Outputs/bdowox1GBIFr24GBIFf9.sqlite\n"
     ]
    }
   ],
   "source": [
    "species_id = 'bdowox1'\n",
    "summary_name = 'DownyNew'\n",
    "gbif_req_id = 'GBIFr24'\n",
    "gbif_filter_id = 'GBIFf9'\n",
    "workDir = 'T:/temp/'\n",
    "codeDir = 'T:/code/wildlife-wrangler/'\n",
    "inDir = workDir + 'Inputs/'\n",
    "outDir = workDir + 'Outputs/'\n",
    "default_coordUncertainty = 200 # Note above.\n",
    "SRID_dict = {'WGS84': 4326, 'AlbersNAD83': 102008} # Used in file names for output.\n",
    "spdb = outDir + species_id + gbif_req_id + gbif_filter_id + '.sqlite'\n",
    "\n",
    "username = config.gbif_username\n",
    "password = config.gbif_password\n",
    "email = config.gbif_email\n",
    "\n",
    "print(\"Notebook run \" + str(t1))\n",
    "print(spdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "connjup = sqlite3.connect(codeDir + 'parameters.sqlite')\n",
    "cursorjup = connjup.cursor()\n",
    "\n",
    "# Get some variables\n",
    "years = connjup.execute(\"\"\"SELECT years_range \n",
    "                           FROM gbif_requests WHERE request_id = '{0}'\"\"\".format(gbif_req_id)).fetchone()[0]\n",
    "gap_id = connjup.execute(\"\"\"SELECT gap_id\n",
    "                            FROM species_concepts WHERE species_id = '{0}'\"\"\".format(species_id)).fetchone()[0]\n",
    "common_name = connjup.execute(\"\"\"SELECT common_name\n",
    "                                 FROM species_concepts WHERE species_id = '{0}'\"\"\".format(species_id)).fetchone()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Concept\n",
    "Display information on the species from the parameters.sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bcb_id': None,\n",
      " 'breeding_months': None,\n",
      " 'common_name': 'downy woodpecker',\n",
      " 'detection_distance_meters': 200,\n",
      " 'ebird_id': None,\n",
      " 'end_year': None,\n",
      " 'error_tolerance': 20,\n",
      " 'fws_id': None,\n",
      " 'gap_id': None,\n",
      " 'gbif_id': '9149595',\n",
      " 'geometry': None,\n",
      " 'itis_tsn': None,\n",
      " 'lumped_into': None,\n",
      " 'migratory': '0',\n",
      " 'notes': 'Birdlife was using Dryobates as early as 2014.',\n",
      " 'pad': 1,\n",
      " 'scientific_name': 'Dryobates pubescens',\n",
      " 'species_id': 'bdowox1',\n",
      " 'split_from': 'bdowox0',\n",
      " 'start_year': 2018,\n",
      " 'vetted_date': '3/10/2020',\n",
      " 'vetted_how': 'Consulted avibase.',\n",
      " 'vetted_who': 'N. Tarr',\n",
      " 'wintering_months': None}\n"
     ]
    }
   ],
   "source": [
    "vals = cursorjup.execute(\"SELECT * FROM species_concepts WHERE species_id = '{0}';\".format(species_id)).fetchall()[0]\n",
    "cols = [x[1] for x in cursorjup.execute(\"PRAGMA table_info('species_concepts')\").fetchall()]\n",
    "pprint.pprint(dict(zip(cols, vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters\n",
    "Display the parameters of the request filter set.  These are deployed during the step where records are retrieved from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE REQUEST FILTER SET\n",
      "request_id                            GBIFr24\n",
      "source                                   GBIF\n",
      "lat_range                                None\n",
      "lon_range                                None\n",
      "years_range                         1999,2020\n",
      "months_range                             1,12\n",
      "geoissue                                False\n",
      "coordinate                               True\n",
      "country                                    US\n",
      "geometry                                 None\n",
      "creator                               N. Tarr\n",
      "notes           Used for tests of large do...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_sql_query(sql=\"SELECT * FROM gbif_requests WHERE request_id = '{0}'\".format(gbif_req_id), con=connjup)\n",
    "print(\"THE REQUEST FILTER SET\")\n",
    "print(df1.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the parameters of the post-request filter set.  These are deployed after the records are retrieved from the API, but before they are stored in the occurrence record sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE POST REQUEST FILTER SET\n",
      "filter_id                                            GBIFf9\n",
      "dataset                                                GBIF\n",
      "institutions_omit                                          \n",
      "collection_codes_omit                                      \n",
      "datasets_omit                                          None\n",
      "has_coordinate_uncertainty                                0\n",
      "max_coordinate_uncertainty                            10000\n",
      "bases_omit                                                 \n",
      "sampling_protocols_omit                                    \n",
      "issues_omit                                            None\n",
      "duplicates_OK                                          True\n",
      "creator                                             N. Tarr\n",
      "notes                         Used for speed tests.  Let...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_sql_query(sql=\"SELECT * FROM gbif_filters WHERE filter_id = '{0}'\".format(gbif_filter_id), con=connjup)\n",
    "print(\"THE POST REQUEST FILTER SET\")\n",
    "print(df2.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter set justification\n",
    "**default_coord_uncertainty:** \n",
    "\n",
    "**years_range:**\n",
    "\n",
    "**months_range:** \n",
    "\n",
    "**geoissue:** \n",
    "\n",
    "**coordinate:** \n",
    "\n",
    "**country:**\n",
    "\n",
    "**geometry:**\n",
    "\n",
    "**collection_codes_omit:** \n",
    "\n",
    "**institutions_omit:** \n",
    "\n",
    "**has_coordinate_uncertainty:** \n",
    "\n",
    "**max_coordinate_uncertainty:** \n",
    "\n",
    "**bases_omit:** \n",
    "\n",
    "**protocols_omit:**\n",
    "\n",
    "**sampling_protocols_omit:** \n",
    "\n",
    "**issues_omit:**\n",
    "\n",
    "**duplicates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIALITE_SECURITY set to relaxed\n",
      "Created occurrence db: 0:00:00.381575\n",
      "Got request params and sorted out geometry constraints: 0:00:00.000992\n",
      "5396880 records available\n",
      "Your download key is  0018669-200221144449610\n",
      "Downloading Darwin Core Archive zip file for this species .....\n",
      "Download file size: 1198031118 bytes\n",
      "On disk at T:/temp/Inputs//0018669-200221144449610.zip\n",
      "Download complete: 0:17:26.145543\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C unsigned long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-90ac893ca3bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m functions.retrieve_gbif_occurrences(codeDir, species_id, inDir, spdb, gbif_req_id, gbif_filter_id, \n\u001b[0;32m      2\u001b[0m                                     \u001b[0mdefault_coordUncertainty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musername\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                    password, email)\n\u001b[0m",
      "\u001b[1;32mT:\\Code\\wildlife-wrangler\\repo_functions.py\u001b[0m in \u001b[0;36mretrieve_gbif_occurrences\u001b[1;34m(codeDir, species_id, inDir, spdb, gbif_req_id, gbif_filter_id, default_coordUncertainty, outDir, summary_name, username, password, email)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;31m# Read the \"occurrence.txt\" file into a Pandas dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[0mread1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mDwCAReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdkey\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.zip'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdwca\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m             \u001b[0mdfRaw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdwca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpd_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'occurrence.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, usecols=keeper_keys)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\occur\\lib\\site-packages\\dwca\\read.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, extensions_to_ignore)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# We have an Archive descriptor that we can use to access data files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m#: An instance of :class:`dwca.files.CSVDataFile` for the core data file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCSVDataFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_working_directory_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: CSVDataFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m#: A list of :class:`dwca.files.CSVDataFile`, one entry for each extension data file , sorted by order of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\occur\\lib\\site-packages\\dwca\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, work_directory, file_descriptor)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# that will make random access faster later on...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         self._line_offsets = _get_all_line_offsets(self._file_stream,\n\u001b[1;32m---> 52\u001b[1;33m                                                    self.file_descriptor.file_encoding)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#: Number of lines to ignore (header lines) in the CSV file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\occur\\lib\\site-packages\\dwca\\files.py\u001b[0m in \u001b[0;36m_get_all_line_offsets\u001b[1;34m(f, encoding)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mline_offsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0moffset\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C unsigned long"
     ]
    }
   ],
   "source": [
    "functions.retrieve_gbif_occurrences(codeDir, species_id, inDir, spdb, gbif_req_id, gbif_filter_id, \n",
    "                                    default_coordUncertainty, outDir, summary_name, username,\n",
    "                                   password, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many records made it through the filters?\n",
    "This is the number that was actually saved in the occurrence record sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_occ= sqlite3.connect(spdb)\n",
    "curs_occ = conn_occ.cursor()\n",
    "record_count = curs_occ.execute(\"SELECT COUNT(occ_id) FROM occurrences WHERE species_id = '{0}'\".format(species_id)).fetchone()\n",
    "print(str(record_count[0]) + \" records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there duplicate records left?\n",
    "Duplicates based on latitude, longitude, and date-time should of been removed, with the record with the highest individualCount retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups0 = curs_occ.execute(\"\"\"SELECT count(occ_id) FROM occurrences WHERE occ_id NOT IN (SELECT occ_id FROM occurrences GROUP BY latitude, longitude, occurrenceDate HAVING max(IndividualCount));\"\"\").fetchall()\n",
    "print(str(dups0[0][0]) + ' duplicate records retained based on xy coordinate and date-time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = cursorjup.execute(\"SELECT table_name FROM table_descriptions\").fetchall()\n",
    "tables = [x[0] for x in tables]\n",
    "\n",
    "filter_sets = [gbif_req_id, gbif_filter_id]\n",
    "\n",
    "sources = []\n",
    "for s in filter_sets:\n",
    "    s = s.strip()\n",
    "    for tab in tables:\n",
    "        columns = cursorjup.execute(\"SELECT column_name FROM column_descriptions WHERE table_name = '{0}'\".format(tab)).fetchall()\n",
    "        columns = [x[0] for x in columns]\n",
    "        for col in columns:\n",
    "            try:\n",
    "                a = cursorjup.execute(\"SELECT source FROM {1} WHERE {2} = '{0}'\".format(s, tab, col)).fetchone()[0]\n",
    "                sources.append(a)\n",
    "            except:\n",
    "                pass\n",
    "print(list(set(sources))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = pd.read_sql(sql=\"SELECT * FROM pre_filter_source_counts;\", con=conn_occ)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = cursorjup.execute(\"SELECT table_name FROM table_descriptions\").fetchall()\n",
    "tables = [x[0] for x in tables]\n",
    "\n",
    "filter_sets = [gbif_req_id, gbif_filter_id]\n",
    "\n",
    "sources = []\n",
    "for s in filter_sets:\n",
    "    s = s.strip()\n",
    "    for tab in tables:\n",
    "        columns = cursorjup.execute(\"SELECT column_name FROM column_descriptions WHERE table_name = '{0}'\".format(tab)).fetchall()\n",
    "        columns = [x[0] for x in columns]\n",
    "        for col in columns:\n",
    "            try:\n",
    "                a = cursorjup.execute(\"SELECT source FROM {1} WHERE {2} = '{0}'\".format(s, tab, col)).fetchone()[0]\n",
    "                sources.append(a)\n",
    "            except:\n",
    "                pass\n",
    "print(list(set(sources))[0])\n",
    "\n",
    "sql = \"SELECT institutionCode, collectionCode, datasetName, COUNT(occ_id) FROM occurrences GROUP BY institutionCode, collectionCode, datasetName;\"\n",
    "sources = pd.read_sql(sql=sql, con=conn_occ)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bases\n",
    "#### Pre-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = pd.read_sql(sql=\"SELECT value as basisOfRecord, count FROM pre_filter_value_counts WHERE attribute = 'bases';\", con=conn_occ)\n",
    "print(bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT basisOfRecord, COUNT(occ_id) as count FROM occurrences GROUP BY basisOfRecord;\"\n",
    "bases = pd.read_sql(sql=sql, con=conn_occ)\n",
    "print(bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocols\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protocol = pd.read_sql(sql=\"SELECT value as samplingProtocol, count FROM pre_filter_value_counts WHERE attribute = 'samplingProtocols';\", con=conn_occ)\n",
    "print(protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT samplingProtocol, COUNT(occ_id) as count FROM occurrences GROUP BY samplingProtocol;\"\n",
    "print(pd.read_sql(sql=sql, con=conn_occ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iss = pd.read_sql(sql=\"SELECT value as issues, count FROM pre_filter_value_counts WHERE attribute = 'issues';\", con=conn_occ)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(iss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT issue, COUNT(occ_id) as count FROM occurrences GROUP BY issue;\"\n",
    "print(pd.read_sql(sql=sql, con=conn_occ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptions of filtered records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp1 = {'file': '{0}{1}_polygons'.format(outDir, summary_name), 'column': None,\n",
    "        'alias': 'Occurrence records', 'drawbounds': True, 'linewidth': .75, 'linecolor': 'k',\n",
    "        'fillcolor': None, 'marker':'o'}\n",
    "\n",
    "# Display occurrence polygons\n",
    "map_these=[shp1]\n",
    "    \n",
    "title=\"{1} ({0})\".format(years, common_name)\n",
    "functions.MapShapefilePolygons(map_these=map_these, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_years = [int(x[0]) for x in curs_occ.execute(\"SELECT strftime('%Y', occurrenceDate) FROM occurrences\").fetchall()]\n",
    "years = connjup.execute(\"SELECT years_range FROM gbif_requests WHERE request_id = '{0}'\".format(gbif_req_id)).fetchone()[0]\n",
    "years = years.split(',')\n",
    "yearsrng = list(range(int(years[0]), int(years[1]), 1))\n",
    "binsnum = int(years[1]) - int(years[0])\n",
    "plt.hist(occ_years, bins=binsnum)\n",
    "plt.ylabel(\"number of occurrences\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.xticks(yearsrng, rotation=90)\n",
    "plt.title(\"Occurrences per Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Months represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_months = [int(x[0]) for x in curs_occ.execute(\"SELECT strftime('%m', occurrenceDate) FROM occurrences\").fetchall()]\n",
    "plt.hist(occ_months, bins=range(1, 14), color=\"g\")\n",
    "plt.ylabel(\"number of occurrences\")\n",
    "plt.xlabel(\"month\")\n",
    "plt.xticks(range(1, 13))\n",
    "plt.title(\"Occurrences per Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of coordinate uncertainty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occ_cert = [int(x[0]) for x in curs_occ.execute(\"SELECT coordinateUncertaintyInMeters FROM occurrences\").fetchall()]\n",
    "maxi = np.max(occ_cert)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.hist(occ_cert, bins=50, color=\"r\")\n",
    "plt.xticks(range(0, maxi, int(maxi/50)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"coordinate uncertainty\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_max = 2000\n",
    "occ_cert2 = [x for x in occ_cert if x <= rng_max]\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(occ_cert2, bins=30, color=\"m\", align='mid')\n",
    "plt.xticks(range(0, rng_max + 100, int(rng_max/30.)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"coordinate uncertainty\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishment means reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment = curs_occ.execute(\"SELECT vals FROM unique_values WHERE field = 'establishment' AND step = 'filter';\").fetchall()[0]\n",
    "for est in establishment:\n",
    "    est = est.replace('[', '').strip().replace(']', '').replace(\"'\", \"\")\n",
    "    print(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification qualifiers included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quali = curs_occ.execute(\"SELECT DISTINCT vals FROM unique_values WHERE field = 'IDqualifier' AND step = 'filter';\").fetchall()[0]\n",
    "for q in quali:\n",
    "    q = q.replace('[', '').strip().replace(']', '').replace(\"'\", \"\")\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remarks = curs_occ.execute(\"SELECT DISTINCT remarks FROM occurrences;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes returned for the records in the request (pre-filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fields_summary = pd.read_sql(\"SELECT * FROM gbif_fields_returned\", conn_occ)#, index_col='index')\n",
    "fields_summary.index.name = 'Field'\n",
    "pd.set_option('display.max_rows', 250)\n",
    "print(fields_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = datetime.now()\n",
    "print(t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
