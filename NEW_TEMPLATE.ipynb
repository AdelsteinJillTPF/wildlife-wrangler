{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Specify some paths and names in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "configDir = \"T:/Data/\"  # Path to folder where you saved your wildlifeconfig file.\n",
    "filter_set_json = \"\"\n",
    "taxon_json = \"\"\n",
    "query_name = 'withRTest1'\n",
    "use_eBird = True\n",
    "use_GBIF = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBIF request method\n",
    "Whether to get records from GBIF in a darwin core archive.  \"False\" uses the GBIF API, which has limitations that may be important.  \"True\" requests results be emailed in a darwin core archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dwca = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important, temporary bug fix\n",
    "There is a bug with mpl_toolkits, the following code is a temporary fix until they resolve the bug.\n",
    "https://stackoverflow.com/questions/52911232/basemap-library-using-anaconda-jupyter-notebooks-keyerror-proj-lib/54087410#54087410\n",
    "\n",
    "os.environ['PROJ_LIB'] should be set equal to something like \"c:/Users/jramone/AppData/Local/Continuum/miniconda3/envs/wrangler/Library/share\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['PROJ_LIB'] = \"c:/Users/nmtarr/AppData/Local/Continuum/miniconda3/envs/wrangler/Library/share\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to fill out in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run 2021-03-30 16:55:33.973725\n",
      "T:/Occurrence_Records/withRTest1.sqlite\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.width', 600)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "from IPython.display import Image\n",
    "from pygbif import occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "t1 = datetime.now()\n",
    "import sys\n",
    "sys.path.append(configDir)\n",
    "import wranglerconfig as config\n",
    "codeDir = config.codeDir\n",
    "sys.path.append(codeDir)\n",
    "import wrangler_functions as functions\n",
    "working_directory = config.workDir\n",
    "output_database = working_directory + query_name + '.sqlite'\n",
    "username = config.gbif_username\n",
    "password = config.gbif_password\n",
    "email = config.gbif_email\n",
    "EBD_file = config.EBD_file\n",
    "print(\"Notebook run \" + str(t1))\n",
    "print(output_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify a taxon\n",
    "GBIF and eBird Basic Dataset can currently be accessed.  Specificy the appropriate identifiers in the cell below.\n",
    "\n",
    "NOTE -- cautions about taxon concept mismatches and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_taxon_id = \"TestCuckoo\"\n",
    "gbif_id = 2496287 \n",
    "ebird_id = \"Yellow-billed Cuckoo\"\n",
    "detection_distance_m = 200\n",
    "taxon_polygon = \"POLYGON ((-84.09680233298448 36.69265225442667, -84.07962135716329 34.5561660300382, -84.07962135716329 34.5561660300382, -80.25685423694925 34.65515526072436, -81.15026497965096 36.71331438415306, -84.09680233298448 36.69265225442667))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EBIRD_ID': 'Yellow-billed Cuckoo',\n",
      " 'GBIF_ID': 2496287,\n",
      " 'ID': 'TestCuckoo',\n",
      " 'TAXON_EOO': 'POLYGON ((-84.09680233298448 36.69265225442667, '\n",
      "              '-84.07962135716329 34.5561660300382, -84.07962135716329 '\n",
      "              '34.5561660300382, -80.25685423694925 34.65515526072436, '\n",
      "              '-81.15026497965096 36.71331438415306, -84.09680233298448 '\n",
      "              '36.69265225442667))',\n",
      " 'detection_distance_m': 200}\n"
     ]
    }
   ],
   "source": [
    "# If a json was provided, use it, otherwise create a new one with info that was provided.\n",
    "if taxon_json == \"\":\n",
    "    # Build a species dictionary\n",
    "    taxon_info = {\"ID\": your_taxon_id, \"GBIF_ID\": gbif_id, \"EBIRD_ID\": ebird_id, \"detection_distance_m\": detection_distance_m,\n",
    "                  \"TAXON_EOO\": taxon_polygon}\n",
    "\n",
    "    # Save as json object\n",
    "    out_file = open(working_directory + your_taxon_id + \".json\", \"w\")  \n",
    "    json.dump(taxon_info, out_file) \n",
    "    out_file.close() \n",
    "\n",
    "if taxon_json != \"\":\n",
    "    with open(taxon_json, \"r\") as f:\n",
    "        taxon_info = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "pprint.pprint(taxon_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify a filter set\n",
    "Fill out this section to specify how you want records filtered and cleaned.  Alternatively, you can load a filter set here by specifying a path in the first cell of this notebook.  \n",
    "\n",
    "To skip a filter, enter \"None\" without the quotation marks or \"\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_set_name = \"test_filters_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date limits\n",
    "Enter year and month ranges.  For example, years_range = 2015,2017 and months_range = 3,6\n",
    "\n",
    "Justification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_range = \"2015,2020\"\n",
    "months_range = \"9,11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"US\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding box\n",
    "Coordinates should correspond to WGS84 (EPSG:4326).  Don't use this option if you specify a query polygon below.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_range = \"27,41\"\n",
    "lon_range = \"-89,-75\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxon EOO\n",
    "True or False whether you want to apply the taxon EOO to the filtering.  If True, removes records with centroids outside of the extent of occurrence geometry you provided in taxon_info.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_taxon_geometry = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geoissue\n",
    "Are GBIF records with noted geoissues OK to include? GBIF only.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoissue = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections\n",
    "List collection codes that you'd like to omit. GBIF only.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_codes_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institutions\n",
    "List institution codes that you'd like to omit. GBIF only\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "List datasets that you'd like to omit.\n",
    "#### ebird project here?\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate uncertainty\n",
    "Do you want to remove records without coordinate uncertainty (1) or leave them in the data set (0)?  Note that eBird records in GBIF (EOD) do not have this and neither do data in the EBD dataset.  With the EBD, the length of traveling counts is used as a surrogate value /??!!!!!?????.  \n",
    "\n",
    "max_coordinate_uncertainty must be an integer greater than 0.\n",
    "\n",
    "default_coordUncertainty -- coordinateUncertaintyInMeters is often not provided.  Here is an option to use a default.  If you don't want anything entered, set this equal to False (boolean, not string).\n",
    "\n",
    "A maximum for coordinate uncertainty can also be set in meters.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_coordinate_uncertainty = True\n",
    "default_coordUncertainty = 1000\n",
    "max_coordinate_uncertainty = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases\n",
    "List bases of records that you want to omit.  GBIF only.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling protocols\n",
    "List sampling protocols that you would like to omit.  Options for eBird are ........._______\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_protocols_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "List issues that you want to omit.  GBIF only.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_omit = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry (polygons)\n",
    "Specify polygons to use for spatial filtering.  Records with coordinates outside of the polygons will be removed.  You can specify a geometry for the query and one for the species.  The species geometry is included to facilitate better handling of taxonomic issues.  If both are provided, the intersection is calculated and used as the filter.  The format should be well known text in WGS84 (EPSG 4326), and very importantly, vertices need to be listed counter-clockwise.  See the ccw_wkt_from_shp() function in wrangler functions for help.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_polygon = \"POLYGON ((-82.74809573102132 36.96082629937069, -85.0932989306133 35.63154639485496, -81.0987220521874 33.56697226279766, -79.4235769096217 36.34054727735634, -79.4235769096217 36.34054727735634, -82.74809573102132 36.96082629937069))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "Specify whether duplicates on latitude, longitude, and date should be included.\n",
    "\n",
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_OK = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save or load the filter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bases_omit': None,\n",
      " 'collection_codes_omit': None,\n",
      " 'country': 'US',\n",
      " 'datasets_omit': None,\n",
      " 'default_coordUncertainty': 1000,\n",
      " 'duplicates_OK': False,\n",
      " 'geoissue': False,\n",
      " 'get_dwca': True,\n",
      " 'has_coordinate_uncertainty': True,\n",
      " 'institutions_omit': None,\n",
      " 'issues_omit': None,\n",
      " 'lat_range': '27,41',\n",
      " 'lon_range': '-89,-75',\n",
      " 'max_coordinate_uncertainty': 10000,\n",
      " 'months_range': '9,11',\n",
      " 'name': 'test_filters_1',\n",
      " 'query_polygon': 'POLYGON ((-82.74809573102132 36.96082629937069, '\n",
      "                  '-85.0932989306133 35.63154639485496, -81.0987220521874 '\n",
      "                  '33.56697226279766, -79.4235769096217 36.34054727735634, '\n",
      "                  '-79.4235769096217 36.34054727735634, -82.74809573102132 '\n",
      "                  '36.96082629937069))',\n",
      " 'sampling_protocols_omit': None,\n",
      " 'use_taxon_geometry': True,\n",
      " 'years_range': '2015,2020'}\n"
     ]
    }
   ],
   "source": [
    "if filter_set_json == \"\":\n",
    "    # Build a filter set dictionary\n",
    "    filter_set = {\"name\": filter_set_name, \"query_polygon\": query_polygon, \"issues_omit\": issues_omit,\n",
    "                  \"sampling_protocols_omit\": sampling_protocols_omit, \"bases_omit\": bases_omit,\n",
    "                  \"has_coordinate_uncertainty\": has_coordinate_uncertainty, \"geoissue\": geoissue,\n",
    "                  \"default_coordUncertainty\": default_coordUncertainty,\n",
    "                  \"max_coordinate_uncertainty\": max_coordinate_uncertainty,\n",
    "                  \"datasets_omit\": datasets_omit, \"collection_codes_omit\": collection_codes_omit,\n",
    "                  \"institutions_omit\": institutions_omit, \"geoissue\": geoissue, \"use_taxon_geometry\": use_taxon_geometry,\n",
    "                  \"lat_range\": lat_range, \"lon_range\": lon_range, \"country\": country, \n",
    "                  \"years_range\": years_range, \"months_range\": months_range, \"duplicates_OK\": duplicates_OK, \"get_dwca\": get_dwca}\n",
    "    \n",
    "    # Replace empty strings with None\n",
    "    for x in filter_set.keys():\n",
    "        if filter_set[x] == \"\":\n",
    "            filter_set[x] = None\n",
    "    \n",
    "    # Save as json object\n",
    "    with open(working_directory + filter_set_name + \".json\", \"w\") as f:\n",
    "        json.dump(filter_set, f) \n",
    "        f.close()\n",
    "        \n",
    "if filter_set_json != \"\":\n",
    "    with open(filter_set_json, \"r\") as f:\n",
    "        filter_set = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "pprint.pprint(filter_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output database\n",
    "functions.build_output_database(output_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized fields returned: 0:00:00.203061\n",
      "Got request params and sorted out geometry constraints: 0:00:00.140599\n",
      "2021 records available\n",
      "Your download key is  0236503-200613084148143\n",
      "Downloading Darwin Core Archive zip file for this species .....\n",
      "Download file size: 4264636 bytes\n",
      "On disk at T:/Occurrence_Records//0236503-200613084148143.zip\n",
      "Download complete: 0:01:03.922835\n",
      "Downloaded and loaded records: 0:00:02.005713\n",
      "Summarized fields returned: 0:00:00.124920\n",
      "Stored GBIF Download DOI etc.: 0:00:00.015668\n"
     ]
    }
   ],
   "source": [
    "# Run the appropriate queries\n",
    "if use_eBird == True and use_GBIF == True:\n",
    "    # Run eBird query\n",
    "    ebird_data = functions.get_EBD_records(taxon_info, filter_set, working_directory, EBD_file, query_name)\n",
    "    \n",
    "    # Omit Cornell Lab of Ornithology records when querying GBIF\n",
    "    #  collectionCode like \"EBIRD*\" should be omited.  HOW???>>>>>>>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    '''\n",
    "    filter_set2 = filter_set\n",
    "    filter_set2[\"collection_codes_omit\"] = filter_set[\"collection_codes_omit\"]''' \n",
    "    gbif_data = functions.get_GBIF_records(taxon_info, filter_set, query_name, working_directory, username, password, email) \n",
    "\n",
    "elif use_eBird == True and use_GBIF == False:\n",
    "    # Run eBird query\n",
    "    ebird_data = functions.get_EBD_records(taxon_info, filter_set, working_directory, EBD_file, query_name)\n",
    "\n",
    "elif use_eBird == False and use_GBIF == True:\n",
    "    # Run GBIF query\n",
    "    gbif_data = functions.get_GBIF_records(taxon_info, filter_set, query_name, working_directory, username, password, email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebird_data.head()\n",
    "ebird_data.to_csv(\"T:/Temp/ebird.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_data.head()\n",
    "gbif_data.to_csv(\"T:/Temp/gbif.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wrangler_functions' has no attribute 'apply_filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fd01647e6853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Filter out records with undesirable values, locations, and/or duplication.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_filters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mebird_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgbif_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtaxon_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wrangler_functions' has no attribute 'apply_filters'"
     ]
    }
   ],
   "source": [
    "# Filter out records with undesirable values, locations, and/or duplication.\n",
    "functions.apply_filters(ebird_data, gbif_data, filter_set, taxon_info, working_directory, query_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer points and make a shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to output database, record species and filter information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_occ= sqlite3.connect(output_database)\n",
    "curs_occ = conn_occ.cursor()\n",
    "pd.DataFrame(taxon_info.values(), taxon_info.keys()).to_sql(name='taxon_concept', con=conn_occ, if_exists='replace')\n",
    "pd.DataFrame(filter_set.values(), filter_set.keys()).to_sql(name='filter_set', con=conn_occ, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many records made it through the filters?\n",
    "This is the number of species occurrence records that were actually saved in the occurrence record sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_count = curs_occ.execute(\"SELECT COUNT(occ_id) FROM occurrence_records WHERE taxon_id = '{0}'\".format(taxon_id)).fetchone()\n",
    "print(str(record_count[0]) + \" records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = cursorjup.execute(\"SELECT table_name FROM table_descriptions\").fetchall()\n",
    "tables = [x[0] for x in tables]\n",
    "\n",
    "filter_sets = [gbif_req_id, gbif_filter_id]\n",
    "\n",
    "sources = []\n",
    "for s in filter_sets:\n",
    "    s = s.strip()\n",
    "    for tab in tables:\n",
    "        columns = cursorjup.execute(\"SELECT column_name FROM column_descriptions WHERE table_name = '{0}'\".format(tab)).fetchall()\n",
    "        columns = [x[0] for x in columns]\n",
    "        for col in columns:\n",
    "            try:\n",
    "                a = cursorjup.execute(\"SELECT source FROM {1} WHERE {2} = '{0}'\".format(s, tab, col)).fetchone()[0]\n",
    "                sources.append(a)\n",
    "            except:\n",
    "                pass\n",
    "print(list(set(sources))[0])\n",
    "\n",
    "sources = pd.read_sql(sql=\"SELECT * FROM pre_filter_source_counts;\", con=conn_occ)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = cursorjup.execute(\"SELECT table_name FROM table_descriptions\").fetchall()\n",
    "tables = [x[0] for x in tables]\n",
    "\n",
    "filter_sets = [gbif_req_id, gbif_filter_id]\n",
    "\n",
    "sources = []\n",
    "for s in filter_sets:\n",
    "    s = s.strip()\n",
    "    for tab in tables:\n",
    "        columns = cursorjup.execute(\"SELECT column_name FROM column_descriptions WHERE table_name = '{0}'\".format(tab)).fetchall()\n",
    "        columns = [x[0] for x in columns]\n",
    "        for col in columns:\n",
    "            try:\n",
    "                a = cursorjup.execute(\"SELECT source FROM {1} WHERE {2} = '{0}'\".format(s, tab, col)).fetchone()[0]\n",
    "                sources.append(a)\n",
    "            except:\n",
    "                pass\n",
    "print(list(set(sources))[0])\n",
    "\n",
    "sql = \"SELECT institutionCode, collectionCode, datasetName, COUNT(occ_id) FROM occurrences GROUP BY institutionCode, collectionCode, datasetName;\"\n",
    "sources = pd.read_sql(sql=sql, con=conn_occ)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bases\n",
    "#### Pre-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = pd.read_sql(sql=\"SELECT value as basisOfRecord, count FROM pre_filter_value_counts WHERE attribute = 'bases';\", con=conn_occ)\n",
    "print(bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT basisOfRecord, COUNT(occ_id) as count FROM occurrences GROUP BY basisOfRecord;\"\n",
    "bases = pd.read_sql(sql=sql, con=conn_occ)\n",
    "print(bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocols\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protocol = pd.read_sql(sql=\"SELECT value as samplingProtocol, count FROM pre_filter_value_counts WHERE attribute = 'samplingProtocols';\", con=conn_occ)\n",
    "print(protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT samplingProtocol, COUNT(occ_id) as count FROM occurrences GROUP BY samplingProtocol;\"\n",
    "print(pd.read_sql(sql=sql, con=conn_occ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues\n",
    "#### Pre-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iss = pd.read_sql(sql=\"SELECT value as issues, count FROM pre_filter_value_counts WHERE attribute = 'issues';\", con=conn_occ)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(iss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT issues, COUNT(occ_id) as count FROM occurrences GROUP BY issues;\"\n",
    "print(pd.read_sql(sql=sql, con=conn_occ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptions of filtered records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp1 = {'file': '{0}{1}_polygons'.format(outDir, summary_name), 'column': None,\n",
    "        'alias': 'Occurrence records', 'drawbounds': True, 'linewidth': .75, 'linecolor': 'magenta',\n",
    "        'fillcolor': None, 'marker':'o'}\n",
    "\n",
    "# Display occurrence polygons\n",
    "map_these=[shp1]\n",
    "    \n",
    "title=\"{1} ({0})\".format(years, common_name)\n",
    "try:\n",
    "    functions.MapShapefilePolygons(map_these=map_these, title=title)\n",
    "except Exception as e:\n",
    "    print(\"Unable to map:  \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_years = [int(x[0]) for x in curs_occ.execute(\"SELECT strftime('%Y', occurrenceDate) FROM occurrences\").fetchall()]\n",
    "years = connjup.execute(\"SELECT years_range FROM gbif_requests WHERE request_id = '{0}'\".format(gbif_req_id)).fetchone()[0]\n",
    "years = years.split(',')\n",
    "yearsrng = list(range(int(years[0]), int(years[1]), 1))\n",
    "binsnum = int(years[1]) - int(years[0])\n",
    "plt.hist(occ_years, bins=binsnum)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.xticks(yearsrng, rotation=90)\n",
    "plt.title(\"Occurrences per Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Months represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_months = [int(x[0]) for x in curs_occ.execute(\"SELECT strftime('%m', occurrenceDate) FROM occurrences\").fetchall()]\n",
    "plt.hist(occ_months, bins=range(1, 14), color=\"g\")\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"month\")\n",
    "plt.xticks(range(1, 13))\n",
    "plt.title(\"Occurrences per Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of coordinate uncertainty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occ_cert = [int(x[0]) for x in curs_occ.execute(\"SELECT coordinateUncertaintyInMeters FROM occurrences\").fetchall()]\n",
    "maxi = np.max(occ_cert)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.hist(occ_cert, bins=50, color=\"r\")\n",
    "plt.xticks(range(0, maxi, int(maxi/50)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_cert = [int(x[0]) for x in curs_occ.execute(\"SELECT coordinateUncertaintyInMeters FROM occurrences\").fetchall()]\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.boxplot(occ_cert, vert=False)\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_max = 2000\n",
    "occ_cert2 = [x for x in occ_cert if x <= rng_max]\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(occ_cert2, bins=30, color=\"m\", align='mid')\n",
    "plt.xticks(range(0, rng_max + 100, int(rng_max/30.)), rotation=90)\n",
    "plt.ylabel(\"number of records\")\n",
    "plt.xlabel(\"meters\")\n",
    "plt.title(\"Coordinate Uncertainties Below 2km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishment means reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment = curs_occ.execute(\"SELECT vals FROM unique_values WHERE field = 'establishment' AND step = 'filter';\").fetchall()[0]\n",
    "for est in establishment:\n",
    "    est = est.replace('[', '').strip().replace(']', '').replace(\"'\", \"\")\n",
    "    print(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification qualifiers included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quali = curs_occ.execute(\"SELECT DISTINCT vals FROM unique_values WHERE field = 'IDqualifier' AND step = 'filter';\").fetchall()[0]\n",
    "for q in quali:\n",
    "    q = q.replace('[', '').strip().replace(']', '').replace(\"'\", \"\")\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remarks = curs_occ.execute(\"SELECT DISTINCT remarks FROM occurrences;\").fetchall()\n",
    "if len(remarks) <= 20:\n",
    "    try:\n",
    "        for rem in remarks:\n",
    "            if rem[0][0:1] == ';':\n",
    "                print(rem[0][2:])\n",
    "            else:\n",
    "                print(rem[0])\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"More than 20 remarks, consult the occurrence database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes returned for the records in the request (pre-filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fields_summary = pd.read_sql(\"SELECT * FROM gbif_fields_returned\", conn_occ)#, index_col='index')\n",
    "fields_summary.index.name = 'Field'\n",
    "pd.set_option('display.max_rows', 250)\n",
    "print(fields_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations for records downloaded from GBIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_dwca == True:\n",
    "    print(curs_occ.execute(\"SELECT citations FROM GBIF_download_info\").fetchall()[0][0])\n",
    "else:\n",
    "    print(\"Set 'get_dwca' to True to acquire a list of citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rights associated with records downloaded from GBIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_dwca == True:\n",
    "    print(curs_occ.execute(\"SELECT rights FROM GBIF_download_info\").fetchall()[0][0])\n",
    "else:\n",
    "    print(\"Set 'get_dwca' to True to see the rights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBIF download doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_dwca == True:\n",
    "    doi = curs_occ.execute(\"SELECT doi FROM GBIF_download_info\").fetchall()[0][0]\n",
    "    print(\"https://doi.org/\" + doi)\n",
    "else:\n",
    "    print(\"Set 'get_dwca' to True to perform a search with a doi assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBIF Download Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_dwca == True:\n",
    "    print(curs_occ.execute(\"SELECT download_key FROM GBIF_download_info\").fetchall()[0][0])\n",
    "else:\n",
    "    print(\"Set 'get_dwca' to True to perform a search with a download key assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = datetime.now()\n",
    "print(t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
